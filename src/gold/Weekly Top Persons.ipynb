{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99587cd9-f4b4-4c57-8141-6da5c3f37dcd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "sys.path.append('../libs')\n",
    "\n",
    "import utils\n",
    "\n",
    "src_catalog = dbutils.widgets.get('src_catalog')\n",
    "src_schema = dbutils.widgets.get('src_schema')\n",
    "src_table = dbutils.widgets.get('src_table')\n",
    "src_partition = dbutils.widgets.get('src_partition')\n",
    "\n",
    "tgt_catalog = dbutils.widgets.get('tgt_catalog')\n",
    "tgt_schema = dbutils.widgets.get('tgt_schema')\n",
    "tgt_table = dbutils.widgets.get('tgt_table')\n",
    "tgt_partition = dbutils.widgets.get('tgt_partition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f386fde-dd8c-4941-9fc4-909bebf32258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if utils.table_exists(spark, tgt_catalog, tgt_schema, tgt_table):\n",
    "\n",
    "    last_updated = utils.get_last_partition(spark, tgt_catalog, tgt_schema, tgt_table, tgt_partition)\n",
    "\n",
    "    end_date = utils.get_last_partition(spark, src_catalog, src_schema, src_table, src_partition)\n",
    "\n",
    "    if last_updated == end_date:\n",
    "        print('Table is already up to date')\n",
    "        dbutils.notebook.exit()\n",
    "\n",
    "    start_date = spark.sql(f'''\n",
    "        SELECT \n",
    "            MIN(ref_date) \n",
    "        FROM silver.nytimes.top_stories \n",
    "        WHERE weekofyear(ref_date) = {datetime.strptime(end_date, \"%Y-%m-%d\").date().strftime('%V')}\n",
    "    ''').collect()[0][0]\n",
    "    \n",
    "    print(f'Updating table with data from {start_date} to {end_date}')\n",
    "else:\n",
    "    start_date = utils.get_first_partition(spark, src_catalog, src_schema, src_table, src_partition)\n",
    "    end_date = utils.get_last_partition(spark, src_catalog, src_schema, src_table, src_partition)\n",
    "    print(f'Creating table with data from {start_date} to {end_date}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21959a3-0f3c-49f0-a8dd-f62a8ae5eb63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_upload = spark.sql(f'''\n",
    "  WITH persons AS (\n",
    "    SELECT\n",
    "      ref_date,\n",
    "      weekofyear(ref_date) AS week_number,\n",
    "      explode(ds_persons) AS person\n",
    "    FROM {src_catalog}.{src_schema}.{src_table}\n",
    "    WHERE ref_date >= '{start_date}'\n",
    "  ),\n",
    "\n",
    "  week_period AS (\n",
    "    SELECT\n",
    "      week_number,\n",
    "      MIN(ref_date) AS week_start,\n",
    "      MAX(ref_date) AS week_end\n",
    "    FROM persons\n",
    "    GROUP BY week_number\n",
    "  )\n",
    "\n",
    "  SELECT\n",
    "    p.week_number,\n",
    "    week_start,\n",
    "    week_end,\n",
    "    concat(\n",
    "      split_part(regexp_replace(person, r'\\\\s\\\\(\\\\d\\\\d\\\\d\\\\d\\\\-?.?.?.?.', ''), ',', '2'),\n",
    "      ' ',\n",
    "      split_part(regexp_replace(person, r'\\\\s\\\\(\\\\d\\\\d\\\\d\\\\d\\\\-?.?.?.?.', ''), ',', '1')\n",
    "    ) AS person,\n",
    "    COUNT(*) AS mentions,\n",
    "    week_end AS ref_date\n",
    "  FROM persons p\n",
    "  INNER JOIN week_period ON p.week_number = week_period.week_number\n",
    "  GROUP BY p.week_number, week_start, week_end, person\n",
    "  ORDER BY week_number DESC, mentions DESC\n",
    "'''.format(\n",
    "  src_catalog = src_catalog, \n",
    "  src_schema = src_schema, \n",
    "  src_table = src_table, \n",
    "  start_date = start_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "734e191c-afa3-4fd2-b7fd-e05b60a9f33a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df_upload.write\n",
    "    .partitionBy('{tgt_partition}')\n",
    "    .format('delta')\n",
    "    .mode('overwrite')\n",
    "    .option('replaceWhere', f'{tgt_partition} >= \"{start_date}\"')\n",
    "    .saveAsTable(f'{tgt_catalog}.{tgt_schema}.{tgt_table}')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6248061293038283,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Weekly Top Persons",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
