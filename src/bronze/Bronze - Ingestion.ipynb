{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9e44c7-6e2e-42ee-9102-bc6a0f226a21",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77d9820-8a6f-43df-80d9-7d383bc844ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "schema_name = dbutils.widgets.get('schema_name')\n",
    "database_name = dbutils.widgets.get('database_name')\n",
    "table_name = dbutils.widgets.get('table_name')\n",
    "partition_column = dbutils.widgets.get('partition_column')\n",
    "\n",
    "stream_source = f'/Volumes/raw/{database_name}/{table_name}'\n",
    "stream_checkpoint = f'/Volumes/raw/{database_name}/{table_name}_checkpoint/'\n",
    "stream_schema = StructType(\n",
    "    [\n",
    "        StructField(\"abstract\", StringType(), True),\n",
    "        StructField(\"byline\", StringType(), True),\n",
    "        StructField(\"created_date\", StringType(), True),\n",
    "        StructField(\"des_facet\", ArrayType(StringType(), True), True),\n",
    "        StructField(\"geo_facet\", ArrayType(StringType(), True), True),\n",
    "        StructField(\"item_type\", StringType(), True),\n",
    "        StructField(\"kicker\", StringType(), True),\n",
    "        StructField(\"material_type_facet\", StringType(), True),\n",
    "        StructField(\n",
    "            \"multimedia\",\n",
    "            ArrayType(\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"caption\", StringType(), True),\n",
    "                        StructField(\"copyright\", StringType(), True),\n",
    "                        StructField(\"format\", StringType(), True),\n",
    "                        StructField(\"height\", LongType(), True),\n",
    "                        StructField(\"subtype\", StringType(), True),\n",
    "                        StructField(\"type\", StringType(), True),\n",
    "                        StructField(\"url\", StringType(), True),\n",
    "                        StructField(\"width\", LongType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            True,\n",
    "        ),\n",
    "        StructField(\"org_facet\", ArrayType(StringType(), True), True),\n",
    "        StructField(\"per_facet\", ArrayType(StringType(), True), True),\n",
    "        StructField(\"published_date\", StringType(), True),\n",
    "        StructField(\"section\", StringType(), True),\n",
    "        StructField(\"short_url\", StringType(), True),\n",
    "        StructField(\"subsection\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"updated_date\", StringType(), True),\n",
    "        StructField(\"uri\", StringType(), True),\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField('ref_date', StringType(), True),\n",
    "        StructField(\"ingestion_date\", StringType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64ea4da-9cae-478c-9e1f-890f0bc3c40f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Verify Table"
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify if a table already exists\n",
    "def table_exists(schema_name, database_name, table_name):\n",
    "    spark.catalog.setCurrentCatalog(schema_name)\n",
    "    \n",
    "    return spark.catalog.tableExists(f'{database_name}.{table_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba37aa12-0c15-44b4-b669-288da003e9b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Ingestion"
    }
   },
   "outputs": [],
   "source": [
    "def ingestion(df):\n",
    "    df = df.withColumn('ingestion_date', lit(datetime.now().strftime('%Y-%m-%d')))\n",
    "    df.createOrReplaceTempView('payload')\n",
    "\n",
    "    spark.sql(f'''\n",
    "        INSERT INTO {schema_name}.{database_name}.{table_name}\n",
    "        REPLACE USING ({partition_column})\n",
    "        SELECT * FROM payload\n",
    "    ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb63afa-edf1-41a3-aa8b-857d641ab243",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Table creation"
    }
   },
   "outputs": [],
   "source": [
    "if table_exists(schema_name, database_name, table_name):\n",
    "    print(f'Table {table_name} already exists in {schema_name}.{database_name}')\n",
    "else:\n",
    "    print(f'Creating {table_name} table in {schema_name}.{database_name}...')\n",
    "\n",
    "    df_creation = spark.createDataFrame([], schema=stream_schema)\n",
    "\n",
    "    (df_creation.write\n",
    "     .format('delta')\n",
    "     .mode('overwrite')\n",
    "     .partitionBy('ref_date')\n",
    "     .saveAsTable(f'{schema_name}.{database_name}.{table_name}')\n",
    "    )\n",
    "\n",
    "    print(F'{table_name} table created successfully at {schema_name}.{database_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e6b1b81-55e2-4a0b-8ca0-ee4723e998fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading Stream"
    }
   },
   "outputs": [],
   "source": [
    "df_stream = (spark.readStream\n",
    "                .format('cloudFiles')\n",
    "                .option('cloudFiles.format', 'json')\n",
    "                .schema(stream_schema)\n",
    "                .load(stream_source)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfaf03b2-eea0-4364-8124-4372145127c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Writing Stream"
    }
   },
   "outputs": [],
   "source": [
    "stream = (df_stream.writeStream\n",
    "    .option(\"checkpointLocation\", stream_checkpoint)\n",
    "    .foreachBatch(\n",
    "        lambda df, epoch_id: ingestion(df)\n",
    "    )\n",
    "    .trigger(availableNow=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cb1c7b5-b017-470a-b005-112b6c182f56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stream execution"
    }
   },
   "outputs": [],
   "source": [
    "start = stream.start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8487397785821354,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze - Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
